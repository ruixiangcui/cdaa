{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "\n",
    "def represent_text(text,n):\n",
    "    # Extracts all character 'n'-grams from  a 'text'\n",
    "    if n>0:\n",
    "        tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    frequency = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        frequency[token] += 1\n",
    "    return frequency\n",
    "\n",
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts\n",
    "\n",
    "def extract_vocabulary(texts,n,ft):\n",
    "    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n",
    "    occurrences=defaultdict(int)\n",
    "    \n",
    "    #for i in grams:\n",
    "        \n",
    "    for (text,label) in texts:\n",
    "        text_occurrences=represent_text(text,n)\n",
    "        for ngram in text_occurrences:\n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram]+=text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram]=text_occurrences[ngram]\n",
    "\n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i]>=ft:\n",
    "            vocabulary.append(i)\n",
    "    return vocabulary\n",
    "\n",
    "# Lexical Features extraction\n",
    "\n",
    "def LexicalFeatures(docs):\n",
    "    fvs_lexical = np.zeros((len(docs), 3), np.float64)\n",
    "    fvs_punct = np.zeros((len(docs), 6), np.float64)\n",
    "    word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    for e, ch_text in enumerate(docs):\n",
    "#             print(e,ch_text)\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        tokens = nltk.word_tokenize(ch_text[0].lower())\n",
    "        words = word_tokenizer.tokenize(ch_text[0].lower())\n",
    "#             print(len(words))\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text[0])\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentences])\n",
    "\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    "\n",
    "       # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "        # Punctuations per sentece\n",
    "        fvs_punct[e, 1] = tokens.count('.') / float(len(sentences))\n",
    "        # Semicolons per sentence\n",
    "        fvs_punct[e, 2] = tokens.count(';') / float(len(sentences))\n",
    "        # Colons per sentence\n",
    "        fvs_punct[e, 3] = tokens.count(':') / float(len(sentences))\n",
    "        # Colons per sentence\n",
    "        fvs_punct[e, 4] = tokens.count('!') / float(len(sentences))\n",
    "        # Colons per sentence\n",
    "        fvs_punct[e, 5] = tokens.count('-') / float(len(sentences))\n",
    "        \n",
    "        #print(fvs_punct[e, 0],fvs_punct[e, 1],fvs_punct[e, 2],fvs_punct[e, 3],fvs_punct[e, 4],fvs_punct[e, 5])\n",
    "    max_abs_scaler = preprocessing.Normalizer()\n",
    "    fvs_lexical = max_abs_scaler.fit_transform(fvs_lexical)\n",
    "    fvs_punct = max_abs_scaler.fit_transform(fvs_punct)\n",
    "    return fvs_lexical,fvs_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline word and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "Fitting...\n",
      "Prediciting...\n",
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "Fitting...\n",
      "Prediciting...\n",
      "\t answers saved to file answers-problem00002.json\n",
      "elapsed time: 14.32524299621582\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "path = \"pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02\"\n",
    "outpath = \"outpath\"\n",
    "    \n",
    "# Reading information about the collection\n",
    "infocollection = path+os.sep+'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "\n",
    "for index,problem in enumerate(problems):\n",
    "    \n",
    "    print(problem)\n",
    "\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "\n",
    "    # Building training set\n",
    "    train_docs=[]\n",
    "    for candidate in candidates:\n",
    "        train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "\n",
    "    train_texts = [text for i,(text,label) in enumerate(train_docs)]\n",
    "    train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "\n",
    "    # Building test set\n",
    "    test_docs=read_files(path+os.sep+problem,unk_folder)\n",
    "    test_texts = [text for i,(text,label) in enumerate(test_docs)]\n",
    "        \n",
    "    pipeline = Pipeline(steps=[\n",
    "    ('union', FeatureUnion([\n",
    "        #('word_vec', TfidfVectorizer(sublinear_tf=True,analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3),max_features=10000)),\n",
    "        ('char_vec', TfidfVectorizer(sublinear_tf=True,\n",
    "        analyzer='char',\n",
    "        ngram_range=(5, 5),\n",
    "        max_features=50000)),\n",
    "        #('pos', PosTagMatrix(tokenizer=nltk.word_tokenize)),\n",
    "        ])),\n",
    "    \n",
    "    #('to_dense', DenseTransformer()),\n",
    "\n",
    "    #('lreg', CalibratedClassifierCV(OneVsRestClassifier(LogisticRegression(solver='lbfgs')),cv=5))\n",
    "    #'lreg', LogisticRegression())\n",
    "    ('SVC', CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1,gamma='auto')),cv=5))\n",
    "    #('Naive', CalibratedClassifierCV(OneVsRestClassifier(MultinomialNB()),cv=5))\n",
    "    #('Naive', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    print(\"Fitting...\")\n",
    "    #X_train\n",
    "    pipeline.fit(train_texts, train_labels)\n",
    "    print(\"Prediciting...\")\n",
    "    predictions=pipeline.predict(test_texts)\n",
    "    proba=pipeline.predict_proba(test_texts)\n",
    "        \n",
    "    # Saving output data\n",
    "    out_data=[]\n",
    "    unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "    pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "    \n",
    "    for i,v in enumerate(predictions):\n",
    "        out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "    with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "    print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "print('elapsed time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mac/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# char and word grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "Fitting...\n",
      "Predicting...\n",
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "Fitting...\n",
      "Predicting...\n",
      "\t answers saved to file answers-problem00002.json\n",
      "elapsed time: 158.7294521331787\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "path = \"pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02\"\n",
    "outpath = \"outpath\"\n",
    "\n",
    "pt=0.1\n",
    "    \n",
    "# Reading information about the collection\n",
    "infocollection = path+os.sep+'collection-info.json'\n",
    "problems = []\n",
    "language = []\n",
    "\n",
    "with open(infocollection, 'r') as f:\n",
    "    for attrib in json.load(f):\n",
    "        problems.append(attrib['problem-name'])\n",
    "        language.append(attrib['language'])\n",
    "\n",
    "\n",
    "for index,problem in enumerate(problems):\n",
    "    \n",
    "    print(problem)\n",
    "\n",
    "    # Reading information about the problem\n",
    "    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "    candidates = []\n",
    "    with open(infoproblem, 'r') as f:\n",
    "        fj = json.load(f)\n",
    "        unk_folder = fj['unknown-folder']\n",
    "        for attrib in fj['candidate-authors']:\n",
    "            candidates.append(attrib['author-name'])\n",
    "\n",
    "    # Building training set\n",
    "    train_docs=[]\n",
    "    for candidate in candidates:\n",
    "        train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "\n",
    "    train_texts = [text for i,(text,label) in enumerate(train_docs)]\n",
    "    train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "\n",
    "    # Building test set\n",
    "    test_docs=read_files(path+os.sep+problem,unk_folder)\n",
    "    test_texts = [text for i,(text,label) in enumerate(test_docs)]\n",
    "\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 1),\n",
    "        lowercase=False,\n",
    "        max_features=10000)\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        analyzer='char',\n",
    "        ngram_range=(5, 7),\n",
    "        lowercase=False,\n",
    "        max_features=50000)\n",
    "\n",
    "    train_word_features = word_vectorizer.fit_transform(train_texts)\n",
    "    \n",
    "    test_word_features = word_vectorizer.transform(test_texts)\n",
    "\n",
    "    train_char_features=char_vectorizer.fit_transform(train_texts)\n",
    "    \n",
    "    test_char_features = char_vectorizer.transform(test_texts)\n",
    "    \n",
    "    #test_char_features = test_char_features.astype(float)\n",
    "            \n",
    "    #train_char_features=train_char_features.astype(float)\n",
    "  \n",
    "            \n",
    "    train_fvs_lexical,train_fvs_punct=LexicalFeatures(train_docs)\n",
    "    test_fvs_lexical,test_fvs_punct=LexicalFeatures(test_docs)\n",
    "    \n",
    "    token_data=[]\n",
    "    for i in train_texts:\n",
    "        token_data.append(nltk.word_tokenize(i))\n",
    "\n",
    "    tagged_data=[]\n",
    "    for j in token_data:\n",
    "        tagged_data.append(nltk.pos_tag(j))\n",
    "\n",
    "    # make data with only tags \n",
    "    tags_of_data=[]\n",
    "    for one_tag_data in tagged_data:\n",
    "        tags_of_one=[]\n",
    "        for i in one_tag_data:\n",
    "            tags_of_one.append(i[1])\n",
    "\n",
    "        tags_of_data.append(tags_of_one)\n",
    "    all_pos=[]\n",
    "    #make tags vocabulary\n",
    "    for sublist in tags_of_data:\n",
    "        for item in sublist:\n",
    "            all_pos.append(item)\n",
    "\n",
    "    \n",
    "    all_pos=list(set(all_pos))\n",
    "\n",
    "    tags_of_data=[str(i) for i in tags_of_data]\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,5),vocabulary=all_pos,lowercase=False)\n",
    "    train_data_pos=vectorizer.fit_transform(tags_of_data)\n",
    "    \n",
    "    token_test=[]\n",
    "    for i in test_texts:\n",
    "        token_test.append(nltk.word_tokenize(i))\n",
    "\n",
    "    tagged_test=[]\n",
    "    for j in token_test:\n",
    "        tagged_test.append(nltk.pos_tag(j))\n",
    "    # make data containing only tags \n",
    "    tags_of_test=[]\n",
    "    for one_tag_test in tagged_test:\n",
    "        tags_of_one=[]\n",
    "        for i in one_tag_test:\n",
    "            tags_of_one.append(i[1])\n",
    "        tags_of_one=\" \".join(tags_of_one)\n",
    "        tags_of_test.append(tags_of_one)\n",
    "\n",
    "    tags_of_test=[str(i) for i in tags_of_test]\n",
    "    test_data_pos=vectorizer.fit_transform(tags_of_test)\n",
    "    \n",
    "    max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "    train_char_features = max_abs_scaler.fit_transform(train_char_features)\n",
    "    test_char_features = max_abs_scaler.transform(test_char_features)\n",
    "    \n",
    "    max_abs_scaler_word = preprocessing.MaxAbsScaler()\n",
    "    train_word_features = max_abs_scaler_word.fit_transform(train_word_features)\n",
    "    test_word_features = max_abs_scaler_word.transform(test_word_features)\n",
    "    \n",
    "    max_abs_scaler_pos = preprocessing.MaxAbsScaler()\n",
    "    scaled_train_data_pos = max_abs_scaler_pos.fit_transform(train_data_pos)\n",
    "    scaled_test_data_pos = max_abs_scaler_pos.transform(test_data_pos)\n",
    "     \n",
    "    \n",
    "    # Stack here with lexical and/or POS features:\n",
    "    train_features = hstack([train_char_features, train_word_features])\n",
    "    #train_features = hstack([train_features, train_fvs_lexical])\n",
    "    #train_features = hstack([train_features, train_fvs_punct])\n",
    "    #train_features = hstack([train_features, scaled_train_data_pos])\n",
    "    \n",
    "    #train_features = hstack([scaled_train_data_pos,train_fvs_lexical])\n",
    "    #train_features = hstack([train_features, train_fvs_punct])\n",
    "\n",
    "\n",
    "    #print(train_features[0])\n",
    "\n",
    "    test_features = hstack([test_char_features, test_word_features])\n",
    "    #test_features = hstack([test_features, test_fvs_lexical])\n",
    "    #test_features = hstack([test_features, test_fvs_punct])\n",
    "    #test_features = hstack([test_features, scaled_test_data_pos])\n",
    "    \n",
    "    #test_features = hstack([scaled_test_data_pos, test_fvs_lexical]) \n",
    "    #test_features = hstack([test_features, test_fvs_punct])\n",
    "\n",
    "    #print(test_features.shape)\n",
    "    #stop\n",
    "    \n",
    "\n",
    "    clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1,gamma='auto')),cv=5)\n",
    "    #clf=CalibratedClassifierCV(OneVsRestClassifier(LogisticRegression(solver='lbfgs')),cv=5)\n",
    "    #clf=CalibratedClassifierCV(OneVsRestClassifier(MultinomialNB()),cv=5)\n",
    "    \n",
    "    print(\"Fitting...\")\n",
    "    clf.fit(train_features, train_labels)\n",
    "    print(\"Predicting...\")\n",
    "    \n",
    "    predictions=clf.predict(test_features)\n",
    "    proba=clf.predict_proba(test_features)\n",
    "\n",
    "\n",
    "    # Saving output data \n",
    "    out_data=[]\n",
    "    unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "    pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "    \n",
    "    for i,v in enumerate(predictions):\n",
    "        out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "    with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "        json.dump(out_data, f, indent=4)\n",
    "    print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "print('elapsed time:', time.time() - start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001 Macro-F1: 0.491 Accuracy: 0.629 Precision: 0.469 Recall: 0.676\n",
      "problem00002 Macro-F1: 0.697 Accuracy: 0.81 Precision: 0.68 Recall: 0.75\n",
      "Overall score: 0.594 Overall accuracy: 0.72 Overall precision: 0.574 Overall recall: 0.713\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def eval_measures(gt, pred):\n",
    "    \"\"\"Compute macro-averaged F1-scores, macro-averaged precision, \n",
    "    macro-averaged recall, and micro-averaged accuracy according the ad hoc\n",
    "    rules discussed at the top of this file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt : dict\n",
    "        Ground truth, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    pred : dict\n",
    "        Predicted attribution, where keys indicate text file names\n",
    "        (e.g. `unknown00002.txt`), and values represent\n",
    "        author labels (e.g. `candidate00003`)\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        Macro-averaged F1-score\n",
    "    precision : float\n",
    "        Macro-averaged precision\n",
    "    recall : float\n",
    "        Macro-averaged recall\n",
    "    accuracy : float\n",
    "        Micro-averaged F1-score\n",
    "    \"\"\"\n",
    "\n",
    "    actual_authors = list(gt.values())\n",
    "    encoder = LabelEncoder().fit(['<UNK>'] + actual_authors)\n",
    "\n",
    "    text_ids, gold_authors, silver_authors = [], [], []\n",
    "    for text_id in sorted(gt):\n",
    "        text_ids.append(text_id)\n",
    "        gold_authors.append(gt[text_id])\n",
    "        try:\n",
    "            silver_authors.append(pred[text_id])\n",
    "        except KeyError:\n",
    "            # missing attributions get <UNK>:\n",
    "            silver_authors.append('<UNK>')\n",
    "\n",
    "    assert len(text_ids) == len(gold_authors)\n",
    "    assert len(text_ids) == len(silver_authors)\n",
    "\n",
    "    # replace non-existent silver authors with '<UNK>':\n",
    "    silver_authors = [a if a in encoder.classes_ else '<UNK>' \n",
    "                      for a in silver_authors]\n",
    "\n",
    "    gold_author_ints = encoder.transform(gold_authors)\n",
    "    silver_author_ints = encoder.transform(silver_authors)\n",
    "    # get F1 for individual classes (and suppress warnings):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        labels=list(set(gold_author_ints))\n",
    "        # Exclude the <UNK> class\n",
    "        for x in labels:\n",
    "            #print(encoder.inverse_transform([2]))\n",
    "            if encoder.inverse_transform([x])=='<UNK>':\n",
    "                #print(encoder.inverse_transform([x]))\n",
    "                labels.remove(x)\n",
    "        f1 = f1_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        precision = precision_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        recall = recall_score(gold_author_ints,\n",
    "                  silver_author_ints,\n",
    "                  labels,\n",
    "                  average='macro')\n",
    "        accuracy = accuracy_score(gold_author_ints,\n",
    "                  silver_author_ints)\n",
    "\n",
    "    return f1,precision,recall,accuracy\n",
    "\n",
    "def evaluate(ground_truth_file,predictions_file):\n",
    "    # Calculates evaluation measures for a single attribution problem\n",
    "    gt = {}\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        for attrib in json.load(f)['ground_truth']:\n",
    "            gt[attrib['unknown-text']] = attrib['true-author']\n",
    "    pred = {}\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            if attrib['unknown-text'] not in pred:\n",
    "                pred[attrib['unknown-text']] = attrib['predicted-author']\n",
    "    f1,precision,recall,acc =  eval_measures(gt,pred)\n",
    "    return round(f1,3), round(precision,3), round(recall,3), round(acc,3)\n",
    "\n",
    "def evaluate_all(path_collection,path_answers,path_out):\n",
    "    # Calculates evaluation measures for a PAN-18 collection of attribution problems\n",
    "    infocollection = path_collection+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    data = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "    scores=[];\n",
    "    accscore=[];\n",
    "    recscore=[];\n",
    "    precscore=[];\n",
    "    for problem in problems:\n",
    "        f1,precision,recall,acc=evaluate(path_collection+os.sep+problem+os.sep+'ground-truth.json',\n",
    "                                         path_answers+os.sep+'answers-'+problem+'.json')\n",
    "        accscore.append(acc)\n",
    "        scores.append(f1)\n",
    "        recscore.append(recall)\n",
    "        precscore.append(precision)\n",
    "        data.append({'problem-name': problem, 'macro-f1': round(f1,3), 'macro-precision': round(precision,3),\n",
    "                     'macro-recall': round(recall,3),'macro-acc': round(acc,3)})\n",
    "        print(str(problem),'Macro-F1:',round(f1,3),'Accuracy:',round(acc,3),\n",
    "              'Precision:',round(precision,3),'Recall:',round(recall,3))\n",
    "    overall_acc=sum(accscore)/len(accscore)\n",
    "    overall_score=sum(scores)/len(scores)\n",
    "    overall_prec=sum(precscore)/len(precscore)\n",
    "    overall_rec=sum(recscore)/len(recscore)\n",
    "    # Saving data to output files (out.json and evaluation.prototext)\n",
    "    with open(path_out+os.sep+'out.json', 'w') as f:\n",
    "        json.dump({'problems': data, 'overall_score': round(overall_score,3)}, f, indent=4, sort_keys=True)\n",
    "    print('Overall score:', round(overall_score,3),'Overall accuracy:', round(overall_acc,3),\n",
    "          'Overall precision:', round(overall_prec,3),'Overall recall:', round(overall_rec,3))\n",
    "    #print('Overall score:', round(overall_score,3))\n",
    "    prototext='measure {\\n key: \"mean macro-f1\"\\n value: \"'+str(round(overall_score,3))+'\"\\n}\\n'\n",
    "    with open(path_out+os.sep+'evaluation.prototext', 'w') as f:\n",
    "        f.write(prototext)\n",
    "        \n",
    "def main():\n",
    "      \n",
    "    collection = \"pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02\"\n",
    "    answers = \"outpath\"\n",
    "    outeval = \"outeval\"\n",
    "\n",
    "    evaluate_all(collection,answers,outeval)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ALL:\n",
    "problem00001 Macro-F1: 0.614 Accuracy: 0.695 Precision: 0.588 Recall: 0.752\n",
    "problem00002 Macro-F1: 0.611 Accuracy: 0.619 Precision: 0.633 Recall: 0.683\n",
    "Overall score: 0.612 Overall accuracy: 0.657 Overall precision: 0.61 Overall recall: 0.718\n",
    "\n",
    "Only word and char:\n",
    "problem00001 Macro-F1: 0.583 Accuracy: 0.676 Precision: 0.556 Recall: 0.739\n",
    "problem00002 Macro-F1: 0.611 Accuracy: 0.619 Precision: 0.633 Recall: 0.683\n",
    "Overall score: 0.597 Overall accuracy: 0.648 Overall precision: 0.594 Overall recall: 0.711\n",
    "\n",
    "Word, char and lexical:\n",
    "problem00001 Macro-F1: 0.583 Accuracy: 0.676 Precision: 0.556 Recall: 0.739\n",
    "problem00002 Macro-F1: 0.611 Accuracy: 0.619 Precision: 0.633 Recall: 0.683\n",
    "Overall score: 0.597 Overall accuracy: 0.648 Overall precision: 0.594 Overall recall: 0.711\n",
    "\n",
    "Word, char and POS:\n",
    "problem00001 Macro-F1: 0.614 Accuracy: 0.695 Precision: 0.588 Recall: 0.752\n",
    "problem00002 Macro-F1: 0.611 Accuracy: 0.619 Precision: 0.633 Recall: 0.683\n",
    "Overall score: 0.612 Overall accuracy: 0.657 Overall precision: 0.61 Overall recall: 0.718\n",
    "\n",
    "POS and lexical:\n",
    "problem00001 Macro-F1: 0.181 Accuracy: 0.295 Precision: 0.221 Recall: 0.278\n",
    "problem00002 Macro-F1: 0.327 Accuracy: 0.667 Precision: 0.327 Recall: 0.327\n",
    "Overall score: 0.254 Overall accuracy: 0.481 Overall precision: 0.274 Overall recall: 0.302\n",
    "\n",
    "Only POS:\n",
    "problem00001 Macro-F1: 0.179 Accuracy: 0.257 Precision: 0.214 Recall: 0.271\n",
    "problem00002 Macro-F1: 0.417 Accuracy: 0.714 Precision: 0.44 Recall: 0.427\n",
    "Overall score: 0.298 Overall accuracy: 0.486 Overall precision: 0.327 Overall recall: 0.349\n",
    "\n",
    "POS and lexical (ONLY punctuations):\n",
    "problem00001 Macro-F1: 0.172 Accuracy: 0.267 Precision: 0.168 Recall: 0.271\n",
    "problem00002 Macro-F1: 0.317 Accuracy: 0.619 Precision: 0.324 Recall: 0.31\n",
    "Overall score: 0.244 Overall accuracy: 0.443 Overall precision: 0.246 Overall recall: 0.29\n",
    "\n",
    "POS and lexical (ONLY lexical):\n",
    "problem00001 Macro-F1: 0.192 Accuracy: 0.295 Precision: 0.222 Recall: 0.281\n",
    "problem00002 Macro-F1: 0.417 Accuracy: 0.714 Precision: 0.44 Recall: 0.427\n",
    "Overall score: 0.304 Overall accuracy: 0.504 Overall precision: 0.331 Overall recall: 0.354\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 20 candidate authors\n",
      "\t 140 known texts\n",
      "\t vocabulary size: 5640\n",
      "\t 105 unknown texts\n",
      "['candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00006', 'candidate00006', 'candidate00006', 'candidate00006', 'candidate00006', 'candidate00006', 'candidate00006', 'candidate00007', 'candidate00007', 'candidate00007', 'candidate00007', 'candidate00007', 'candidate00007', 'candidate00007', 'candidate00008', 'candidate00008', 'candidate00008', 'candidate00008', 'candidate00008', 'candidate00008', 'candidate00008', 'candidate00009', 'candidate00009', 'candidate00009', 'candidate00009', 'candidate00009', 'candidate00009', 'candidate00009', 'candidate00010', 'candidate00010', 'candidate00010', 'candidate00010', 'candidate00010', 'candidate00010', 'candidate00010', 'candidate00011', 'candidate00011', 'candidate00011', 'candidate00011', 'candidate00011', 'candidate00011', 'candidate00011', 'candidate00012', 'candidate00012', 'candidate00012', 'candidate00012', 'candidate00012', 'candidate00012', 'candidate00012', 'candidate00013', 'candidate00013', 'candidate00013', 'candidate00013', 'candidate00013', 'candidate00013', 'candidate00013', 'candidate00014', 'candidate00014', 'candidate00014', 'candidate00014', 'candidate00014', 'candidate00014', 'candidate00014', 'candidate00015', 'candidate00015', 'candidate00015', 'candidate00015', 'candidate00015', 'candidate00015', 'candidate00015', 'candidate00016', 'candidate00016', 'candidate00016', 'candidate00016', 'candidate00016', 'candidate00016', 'candidate00016', 'candidate00017', 'candidate00017', 'candidate00017', 'candidate00017', 'candidate00017', 'candidate00017', 'candidate00017', 'candidate00018', 'candidate00018', 'candidate00018', 'candidate00018', 'candidate00018', 'candidate00018', 'candidate00018', 'candidate00019', 'candidate00019', 'candidate00019', 'candidate00019', 'candidate00019', 'candidate00019', 'candidate00019', 'candidate00020', 'candidate00020', 'candidate00020', 'candidate00020', 'candidate00020', 'candidate00020', 'candidate00020']\n",
      "\t 60 texts left unattributed\n",
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "\t language:  en\n",
      "\t 5 candidate authors\n",
      "\t 35 known texts\n",
      "\t vocabulary size: 3176\n",
      "\t 21 unknown texts\n",
      "['candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00001', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00002', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00003', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00004', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005', 'candidate00005']\n",
      "\t 13 texts left unattributed\n",
      "\t answers saved to file answers-problem00002.json\n",
      "elapsed time: 33.87999606132507\n"
     ]
    }
   ],
   "source": [
    "def baseline(path,outpath,n=3,ft=5,pt=0.1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        \n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        \n",
    "        # Building training set\n",
    "        train_docs=[]\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "            \n",
    "        train_texts = [text for i,(text,label) in enumerate(train_docs)]\n",
    "        train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "        \n",
    "        #grams = 1,2\n",
    "        \n",
    "        vocabulary = extract_vocabulary(train_docs,3,ft)\n",
    "        #print(vocabulary)\n",
    "        #vectorizer = CountVectorizer(analyzer='char',ngram_range=(grams),lowercase=False,vocabulary=vocabulary)\n",
    "        vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(3,3),lowercase=False,vocabulary=vocabulary)\n",
    "\n",
    "        # Maybe do a TFIDF-vectorizer aswell :) \n",
    "        #print(train_texts)\n",
    "        train_data = vectorizer.fit_transform(train_texts)\n",
    "        #print(train_data)\n",
    "        train_data = train_data.astype(float)\n",
    "        #print(vectorizer.vocabulary_)\n",
    "\n",
    "        for i,v in enumerate(train_texts):\n",
    "            train_data[i]=train_data[i]/len(train_texts[i])\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_texts), 'known texts')\n",
    "        print('\\t', 'vocabulary size:', len(vocabulary))\n",
    "        \n",
    "        # Building test set\n",
    "        test_docs=read_files(path+os.sep+problem,unk_folder)\n",
    "        test_texts = [text for i,(text,label) in enumerate(test_docs)]\n",
    "        test_data = vectorizer.transform(test_texts)\n",
    "        test_data = test_data.astype(float)\n",
    "        for i,v in enumerate(test_texts):\n",
    "            test_data[i]=test_data[i]/len(test_texts[i])\n",
    "        print('\\t', len(test_texts), 'unknown texts')\n",
    "        \n",
    "        \n",
    "        # Applying SVM\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(train_data)\n",
    "        scaled_test_data = max_abs_scaler.transform(test_data)\n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1,gamma='auto')),cv=5) # change cv for faster runtime\n",
    "        print(train_labels)\n",
    "        clf.fit(scaled_train_data, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data)\n",
    "        proba=clf.predict_proba(scaled_test_data)\n",
    "        \n",
    "        # Reject option (used in open-set cases)\n",
    "        count=0\n",
    "        for i,p in enumerate(predictions):\n",
    "            sproba=sorted(proba[i],reverse=True)\n",
    "            if sproba[0]-sproba[1]<pt:\n",
    "                predictions[i]=u'<UNK>'\n",
    "                count=count+1\n",
    "        print('\\t',count,'texts left unattributed')\n",
    "        \n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    folder = \"pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02\"\n",
    "    outpath = \"outpath\"\n",
    "    \n",
    "    baseline(folder, outpath)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem00001\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4152\n",
      "\t 561 unknown texts\n",
      "\t 195 texts left unattributed\n",
      "\t answers saved to file answers-problem00001.json\n",
      "problem00002\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4176\n",
      "\t 137 unknown texts\n",
      "\t 61 texts left unattributed\n",
      "\t answers saved to file answers-problem00002.json\n",
      "problem00003\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4049\n",
      "\t 211 unknown texts\n",
      "\t 130 texts left unattributed\n",
      "\t answers saved to file answers-problem00003.json\n",
      "problem00004\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4255\n",
      "\t 273 unknown texts\n",
      "\t 173 texts left unattributed\n",
      "\t answers saved to file answers-problem00004.json\n",
      "problem00005\n",
      "\t language:  en\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4070\n",
      "\t 264 unknown texts\n",
      "\t 117 texts left unattributed\n",
      "\t answers saved to file answers-problem00005.json\n",
      "problem00006\n",
      "\t language:  fr\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3963\n",
      "\t 121 unknown texts\n",
      "\t 31 texts left unattributed\n",
      "\t answers saved to file answers-problem00006.json\n",
      "problem00007\n",
      "\t language:  fr\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4056\n",
      "\t 92 unknown texts\n",
      "\t 36 texts left unattributed\n",
      "\t answers saved to file answers-problem00007.json\n",
      "problem00008\n",
      "\t language:  fr\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4100\n",
      "\t 430 unknown texts\n",
      "\t 201 texts left unattributed\n",
      "\t answers saved to file answers-problem00008.json\n",
      "problem00009\n",
      "\t language:  fr\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4082\n",
      "\t 239 unknown texts\n",
      "\t 138 texts left unattributed\n",
      "\t answers saved to file answers-problem00009.json\n",
      "problem00010\n",
      "\t language:  fr\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4086\n",
      "\t 38 unknown texts\n",
      "\t 22 texts left unattributed\n",
      "\t answers saved to file answers-problem00010.json\n",
      "problem00011\n",
      "\t language:  it\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3805\n",
      "\t 139 unknown texts\n",
      "\t 70 texts left unattributed\n",
      "\t answers saved to file answers-problem00011.json\n",
      "problem00012\n",
      "\t language:  it\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3745\n",
      "\t 116 unknown texts\n",
      "\t 50 texts left unattributed\n",
      "\t answers saved to file answers-problem00012.json\n",
      "problem00013\n",
      "\t language:  it\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3569\n",
      "\t 196 unknown texts\n",
      "\t 74 texts left unattributed\n",
      "\t answers saved to file answers-problem00013.json\n",
      "problem00014\n",
      "\t language:  it\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3676\n",
      "\t 46 unknown texts\n",
      "\t 19 texts left unattributed\n",
      "\t answers saved to file answers-problem00014.json\n",
      "problem00015\n",
      "\t language:  it\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3663\n",
      "\t 54 unknown texts\n",
      "\t 26 texts left unattributed\n",
      "\t answers saved to file answers-problem00015.json\n",
      "problem00016\n",
      "\t language:  sp\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3930\n",
      "\t 164 unknown texts\n",
      "\t 58 texts left unattributed\n",
      "\t answers saved to file answers-problem00016.json\n",
      "problem00017\n",
      "\t language:  sp\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 4070\n",
      "\t 112 unknown texts\n",
      "\t 56 texts left unattributed\n",
      "\t answers saved to file answers-problem00017.json\n",
      "problem00018\n",
      "\t language:  sp\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3885\n",
      "\t 238 unknown texts\n",
      "\t 112 texts left unattributed\n",
      "\t answers saved to file answers-problem00018.json\n",
      "problem00019\n",
      "\t language:  sp\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3929\n",
      "\t 450 unknown texts\n",
      "\t 201 texts left unattributed\n",
      "\t answers saved to file answers-problem00019.json\n",
      "problem00020\n",
      "\t language:  sp\n",
      "\t 9 candidate authors\n",
      "\t 63 known texts\n",
      "\t vocabulary size: 3874\n",
      "\t 170 unknown texts\n",
      "\t 86 texts left unattributed\n",
      "\t answers saved to file answers-problem00020.json\n",
      "elapsed time: 213.02093315124512\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def represent_text(text,n):\n",
    "    # Extracts all character 'n'-grams from  a 'text'\n",
    "    if n>0:\n",
    "        tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    frequency = defaultdict(int)\n",
    "    for token in tokens:\n",
    "        frequency[token] += 1\n",
    "    return frequency\n",
    "\n",
    "def read_files(path,label):\n",
    "    # Reads all text files located in the 'path' and assigns them to 'label' class\n",
    "    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n",
    "    texts=[]\n",
    "    for i,v in enumerate(files):\n",
    "        f=codecs.open(v,'r',encoding='utf-8')\n",
    "        texts.append((f.read(),label))\n",
    "        f.close()\n",
    "    return texts\n",
    "\n",
    "def extract_vocabulary(texts,n,ft):\n",
    "    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n",
    "    occurrences=defaultdict(int)\n",
    "    \n",
    "    #for i in grams:\n",
    "        \n",
    "    for (text,label) in texts:\n",
    "        text_occurrences=represent_text(text,n)\n",
    "        for ngram in text_occurrences:\n",
    "            if ngram in occurrences:\n",
    "                occurrences[ngram]+=text_occurrences[ngram]\n",
    "            else:\n",
    "                occurrences[ngram]=text_occurrences[ngram]\n",
    "\n",
    "    vocabulary=[]\n",
    "    for i in occurrences.keys():\n",
    "        if occurrences[i]>=ft:\n",
    "            vocabulary.append(i)\n",
    "    return vocabulary\n",
    "\n",
    "def baseline(path,outpath,n=3,ft=5,pt=0.1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reading information about the collection\n",
    "    infocollection = path+os.sep+'collection-info.json'\n",
    "    problems = []\n",
    "    language = []\n",
    "    with open(infocollection, 'r') as f:\n",
    "        for attrib in json.load(f):\n",
    "            problems.append(attrib['problem-name'])\n",
    "            language.append(attrib['language'])\n",
    "    for index,problem in enumerate(problems):\n",
    "        print(problem)\n",
    "        \n",
    "        # Reading information about the problem\n",
    "        infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n",
    "        candidates = []\n",
    "        with open(infoproblem, 'r') as f:\n",
    "            fj = json.load(f)\n",
    "            unk_folder = fj['unknown-folder']\n",
    "            for attrib in fj['candidate-authors']:\n",
    "                candidates.append(attrib['author-name'])\n",
    "        \n",
    "        # Building training set\n",
    "        train_docs=[]\n",
    "        for candidate in candidates:\n",
    "            train_docs.extend(read_files(path+os.sep+problem,candidate))\n",
    "            \n",
    "        train_texts = [text for i,(text,label) in enumerate(train_docs)]\n",
    "        train_labels = [label for i,(text,label) in enumerate(train_docs)]\n",
    "        \n",
    "        #grams = 1,2\n",
    "        \n",
    "        vocabulary = extract_vocabulary(train_docs,3,ft)\n",
    "        #print(vocabulary)\n",
    "        #vectorizer = CountVectorizer(analyzer='char',ngram_range=(grams),lowercase=False,vocabulary=vocabulary)\n",
    "        vectorizer = TfidfVectorizer(analyzer='char',ngram_range=(3,3),lowercase=False,vocabulary=vocabulary)\n",
    "\n",
    "        # Maybe do a TFIDF-vectorizer aswell :) \n",
    "        #print(train_texts)\n",
    "        train_data = vectorizer.fit_transform(train_texts)\n",
    "        #print(train_data)\n",
    "        train_data = train_data.astype(float)\n",
    "        #print(vectorizer.vocabulary_)\n",
    "\n",
    "        for i,v in enumerate(train_texts):\n",
    "            train_data[i]=train_data[i]/len(train_texts[i])\n",
    "        print('\\t', 'language: ', language[index])\n",
    "        print('\\t', len(candidates), 'candidate authors')\n",
    "        print('\\t', len(train_texts), 'known texts')\n",
    "        print('\\t', 'vocabulary size:', len(vocabulary))\n",
    "        \n",
    "        # Building test set\n",
    "        test_docs=read_files(path+os.sep+problem,unk_folder)\n",
    "        test_texts = [text for i,(text,label) in enumerate(test_docs)]\n",
    "        test_data = vectorizer.transform(test_texts)\n",
    "        test_data = test_data.astype(float)\n",
    "        for i,v in enumerate(test_texts):\n",
    "            test_data[i]=test_data[i]/len(test_texts[i])\n",
    "        print('\\t', len(test_texts), 'unknown texts')\n",
    "        \n",
    "        \n",
    "        # Applying SVM\n",
    "        max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "        scaled_train_data = max_abs_scaler.fit_transform(train_data)\n",
    "        scaled_test_data = max_abs_scaler.transform(test_data)\n",
    "        clf=CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1,gamma='auto')),cv=5) # change cv for faster runtime\n",
    "        clf.fit(scaled_train_data, train_labels)\n",
    "        predictions=clf.predict(scaled_test_data)\n",
    "        proba=clf.predict_proba(scaled_test_data)\n",
    "        \n",
    "        # Reject option (used in open-set cases)\n",
    "        count=0\n",
    "        for i,p in enumerate(predictions):\n",
    "            sproba=sorted(proba[i],reverse=True)\n",
    "            if sproba[0]-sproba[1]<pt:\n",
    "                predictions[i]=u'<UNK>'\n",
    "                count=count+1\n",
    "        print('\\t',count,'texts left unattributed')\n",
    "        \n",
    "        # Saving output data\n",
    "        out_data=[]\n",
    "        unk_filelist = glob.glob(path+os.sep+problem+os.sep+unk_folder+os.sep+'*.txt')\n",
    "        pathlen=len(path+os.sep+problem+os.sep+unk_folder+os.sep)\n",
    "        for i,v in enumerate(predictions):\n",
    "            out_data.append({'unknown-text': unk_filelist[i][pathlen:], 'predicted-author': v})\n",
    "        with open(outpath+os.sep+'answers-'+problem+'.json', 'w') as f:\n",
    "            json.dump(out_data, f, indent=4)\n",
    "        print('\\t', 'answers saved to file','answers-'+problem+'.json')\n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "\n",
    "def main():\n",
    "    \n",
    "    folder = \"pan19-cross-domain-authorship-attribution-training-dataset-2019-01-23\"\n",
    "    outpath = \"outpath\"\n",
    "    \n",
    "    baseline(folder, outpath)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
